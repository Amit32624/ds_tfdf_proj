{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 32-bit",
   "metadata": {
    "interpreter": {
     "hash": "e9ccb85bec2989865684bcdce3c5bc09e306bbb351eb4227040237ee1ce59229"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "3\nC:\\Users\\91720\\Documents\\ds_tfdf_proj\\Dataset-P1\\170.txt\n['Ben studies computers about computers in Computer Computer Lab.', 'Steve teaches at Brown University.', 'Data Scientists computers work on large datasets.']\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "folderpaths = 'C:\\\\Users\\\\91720\\Documents\\\\ds_tfdf_proj\\\\Dataset-P1'\n",
    "# filepaths = glob(os.path.join(folderpaths,'*.txt'))\n",
    "all_txt_files =[]\n",
    "for file in Path(folderpaths).rglob(\"*.txt\"):\n",
    "     all_txt_files.append(file.parent / file.name)\n",
    "    #  print(all_txt_files)\n",
    "# counts the length of the list\n",
    "n_files = len(all_txt_files)\n",
    "print(n_files)\n",
    "all_txt_files.sort()\n",
    "print(all_txt_files[0])\n",
    "\n",
    "\n",
    "all_docs = []\n",
    "for txt_file in all_txt_files:\n",
    "    with open(txt_file) as f:\n",
    "        txt_file_as_string = f.read()\n",
    "    all_docs.append(txt_file_as_string)\n",
    "\n",
    "print(all_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  (0, 8)\t1.6931471805599454\n  (0, 4)\t3.386294361119891\n  (0, 7)\t1.6931471805599454\n  (0, 0)\t1.6931471805599454\n  (0, 13)\t1.6931471805599454\n  (0, 2)\t1.6931471805599454\n  (1, 15)\t1.6931471805599454\n  (1, 3)\t1.6931471805599454\n  (1, 1)\t1.6931471805599454\n  (1, 14)\t1.6931471805599454\n  (1, 12)\t1.6931471805599454\n  (2, 6)\t1.6931471805599454\n  (2, 9)\t1.6931471805599454\n  (2, 10)\t1.6931471805599454\n  (2, 16)\t1.6931471805599454\n  (2, 11)\t1.6931471805599454\n  (2, 5)\t1.6931471805599454\n3\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_df=.65, min_df=1, stop_words=None, use_idf=True, norm=None)\n",
    "transformed_documents = vectorizer.fit_transform(all_docs)\n",
    "print(transformed_documents)\n",
    "\n",
    "transformed_documents_as_array = transformed_documents.toarray()\n",
    "# use this line of code to verify that the numpy array represents the same number of documents that we have in the file list\n",
    "print(len(transformed_documents_as_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "          term     score\n0         work  1.693147\n1           on  1.693147\n2         data  1.693147\n3     datasets  1.693147\n4   scientists  1.693147\n5        large  1.693147\n6   university  0.000000\n7      teaches  0.000000\n8      studies  0.000000\n9        steve  0.000000\n10       about  0.000000\n11          at  0.000000\n12          in  0.000000\n13    computer  0.000000\n14       brown  0.000000\n15         ben  0.000000\n16         lab  0.000000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# make the output folder if it doesn't already exist\n",
    "Path(\"./tf_idf_output\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# construct a list of output file paths using the previous list of text files the relative path for tf_idf_output\n",
    "output_filenames = [str(txt_file).replace(\".txt\", \".csv\").replace(\"txt/\", \"tf_idf_output/\") for txt_file in all_txt_files]\n",
    "\n",
    "# loop each item in transformed_documents_as_array, using enumerate to keep track of the current position\n",
    "for counter, doc in enumerate(transformed_documents_as_array):\n",
    "    # construct a dataframe\n",
    "    tf_idf_tuples = list(zip(vectorizer.get_feature_names(), doc))\n",
    "    one_doc_as_df = pd.DataFrame.from_records(tf_idf_tuples, columns=['term', 'score']).sort_values(by='score', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    # output to a csv using the enumerated value for the filename\n",
    "    one_doc_as_df.to_csv(output_filenames[counter])\n",
    "print(one_doc_as_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = [['AMIT','Amit','Sumit'],['Sumit','Amit','Vineet'],['Cronin','Amit']]\n",
    "term ='Amit'\n",
    "for doc in A:     \n",
    "    # print(doc)\n",
    "    # print(A[0])\n",
    "    if term.lower() in doc:\n",
    "        print(term.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}